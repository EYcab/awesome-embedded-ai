---
layout: default
---

# 嵌入式AI简报 (2019-05-15)

**关注模型压缩、低比特量化、移动端推理加速优化、部署**  
<font>注：PC端微信链接打不开请用手机打开</font>


## 业界新闻


- [阿里开源！轻量级深度学习端侧推理引擎 MNN | 阿里技术](https://mp.weixin.qq.com/s/ozqeNa1MRgy-E-9XhZOcoQ)  
摘要：阿里正式开源轻量级深度学习端侧推理引擎“MNN”。AI科学家贾扬清如此评价道：“与 Tensorflow、Caffe2 等同时覆盖训练和推理的通用框架相比，MNN 更注重在推理时的加速和优化，解决在模型部署的阶段的效率问题，从而在移动端更高效地实现模型背后的业务。这和服务器端 TensorRT 等推理引擎的想法不谋而合。在大规模机器学习应用中，考虑到大规模的模型部署，机器学习的推理侧计算量往往是训练侧计算量的十倍以上，所以推理侧的优化尤其重要。”MNN背后的技术框架如何设计？未来有哪些规划？今天一起来深入了解。  
- [MIT初创公司发布全球首个光子AI芯片原型！独立光学计算硬件，庞大系统成功集成至常规板卡 丨 DeepTech深科技](https://mp.weixin.qq.com/s/hqh9O0RqnZY0SUymn2Gfww)  
摘要：光子芯片完成矩阵乘法所用的时间是最先进的电子芯片的 1/100，Lightelligence CEO 沈亦晨表示，公司计划将该光子芯片提供给一些合作方、潜在客户进行测试，目前国外已经有谷歌、FaceBook、AWS级别，国内 BAT 级别的客户与 Lightelligence 接洽。Lightelligence 生产的是一款通用的 AI 芯片，同时我们也会自研更适合在光子芯片上运算的算法。在应用场景上，沈亦晨设想 Lightelligence 的第一款产品将面向服务器和自动驾驶；  
- [传寒武纪最新产品曝光，今年云端AI芯片市场群雄争霸 | DeepTech深科技](https://mp.weixin.qq.com/s/o_nyKDQDH6QhbVqnUDSBAg)  
摘要：寒武纪的新一代云端 AI 芯片思元/ MLU270 已于今年年初研制成功，基于台积电 16nm 工艺打造，架构代号从上一代的 MLUv01 升级到了 MLUv02，内建视频解码单元 (似乎是专门为视频处理市场配置)。在峰值和功耗上，这颗芯片的表现分别是 int4 256Tops 、 int8 128Tops 和 75w，接近目前的 AI 芯片霸主英伟达推出的最新一代 Tesla T4。PPT 上写着，该产品”与市场类似价位竞品相比，具有显著的性能和性价比优势”。  
- [首发 | 精度、速度、效率、性能全面提升！揭秘谷歌最新一代移动端网络架构MobileNet V3 | 将门创投](https://mp.weixin.qq.com/s/LOUB0h3FvBhKH6QZMGMXKQ)  
摘要：支撑移动端高性能AI的幕后力量！谷歌提出全新高性能MobileNet V3，网络模型搜索与精巧设计的完美结合造就新一代移动端网络架构。再附上知乎：[如何评价google Searching for MobileNetV3？ | 知乎](https://www.zhihu.com/question/323419310)；    
- [重磅丨依图造芯，定名“求索” | 雷锋网](https://mp.weixin.qq.com/s/P9WwChO_ih2QeRty70JKlg)    
摘要：该芯片功耗单路数小于1W、0.75TOPS/W；单芯片50路视频解析，1U可支持200路；自带网络支持，支持虚拟化，支持Docker；支持通用视觉：检测、分类、识别、分割、跟踪等等。同时，吕昊还在发布会现场演示了“求索”芯片性能，他直接架起200路摄像机，通过四块“求索”芯片实时比对现场超过五百位现场观众的人脸。


## 论文

## 开源项目

- [JDAI-CV/dabnn: dabnn is an accelerated binary neural networks inference framework for mobile platform](https://github.com/JDAI-CV/dabnn)  
摘要：二值网络移动端 inference 框架，刚刚开源，比已有唯一一个开源的二值框架最高快 20 多倍，相信可以极大改变二值网络部署的情况，对二值网络感兴趣的小伙伴可以关注一下；
- [精度无损，体积压缩70%以上，百度PaddleSlim为你的模型瘦身 | 机器之心](https://mp.weixin.qq.com/s/oJr5fx6uF5rsguK2rOpVoQ)  
摘要：PaddleSlim 实现了目前主流的网络量化、剪枝、蒸馏三种压缩策略，并可快速配置多种压缩策略组合使用。针对体积已经很小的 MobileNet 模型，在模型效果不损失的前提下实现 70% 以上的体积压缩。

## 博文

- [AI移动端常用汇编指令汇总以及底层算子汇编实现（附带一点点干货） | 知乎](https://zhuanlan.zhihu.com/p/64025085)  
摘要：本文先介绍汇编代码在推理框架中的定位，汇总实践中常用指令，对比ARMv7、ARMv8下的指令形式以及用法，在安卓、ios平台下实现各种卷积的底层实现，包括conv3x3s1、conv1x1s1、depthwise、pooling、ReLU、ReLU6，最后可能会结合一个具体的实例来写；  
- [移动端arm cpu优化学习笔记：一步步优化盒子滤波（Box Filter） | 知乎](https://zhuanlan.zhihu.com/p/64522357)  
摘要：作者从原始的C代码到算法逻辑优化、Neon Intrinsic、Neon汇编，比起最原始的实现至少可以加速6~7倍；  
- [OpenBLAS gemm从零入门 | 知乎](https://zhuanlan.zhihu.com/p/65436463)  



## [往期回顾](https://github.com/ysh329/awesome-embedded-ai)

- [2019-04-27](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-04-27.md)  
- [2019-04-13](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-04-13.md)  
- [2019-03-31](https://github.com/ysh329/awesome-embedded-ai/blob/master/embedded-ai-report/2019-03-31.md)  

----

![wechat_qrcode](../wechat_qrcode.jpg)

Wechat ID: NeuroMem  
Editor: https://github.com/ysh329  
Project: https://github.com/ysh329/awesome-embedded-ai  

----

<a rel="license" href="http://creativecommons.org/licenses/by-sa/2.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/2.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-sa/2.0/">知识共享署名-相同方式共享 2.0 通用许可协议</a>进行许可。
